{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f9fabfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: python-chess in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (0.31.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n"
     ]
    }
   ],
   "source": [
    "!pip install python-chess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d987781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import inspect\n",
    "import pprint\n",
    "import chess\n",
    "from chess.pgn import Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2a1d022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7286d5",
   "metadata": {},
   "source": [
    "# Monte Carlo Tree Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8aea7e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a9db655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (2.6.2)\n",
      "Requirement already satisfied: h5py~=3.1.0 in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: gast==0.4.0 in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.7,>=2.6.0 in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.37.0 in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (from tensorflow) (1.48.2)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (from tensorflow) (3.19.6)\n",
      "Requirement already satisfied: clang~=5.0 in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (from tensorflow) (5.0)\n",
      "Requirement already satisfied: tensorboard<2.7,>=2.6.0 in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (from tensorflow) (1.19.5)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: keras<2.7,>=2.6.0 in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (from tensorflow) (0.15.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: cached-property in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (from h5py~=3.1.0->tensorflow) (1.5.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (59.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (1.35.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (3.3.7)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (from markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow) (4.8.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (1.26.18)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (3.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (2.0.12)\n",
      "Requirement already satisfied: dataclasses in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (from werkzeug>=0.11.15->tensorboard<2.7,>=2.6.0->tensorflow) (0.8)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\soner\\appdata\\roaming\\python\\python36\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14c5d010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a90305",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83e1c086",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Flatten, Concatenate, Conv2D, Dropout\n",
    "from keras.losses import mean_squared_error\n",
    "from keras.models import Model, clone_model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class RandomAgent(object):\n",
    "\n",
    "    def __init__(self, color=1):\n",
    "        self.color = color\n",
    "\n",
    "    def predict(self, board_layer):\n",
    "        return np.random.randint(-5, 5) / 5\n",
    "\n",
    "    def select_move(self, board):\n",
    "        moves = [x for x in board.generate_legal_moves()]\n",
    "        return np.random.choice(moves)\n",
    "\n",
    "\n",
    "class GreedyAgent(object):\n",
    "\n",
    "    def __init__(self, color=-1):\n",
    "        self.color = color\n",
    "\n",
    "    def predict(self, layer_board, noise=True):\n",
    "        layer_board1 = layer_board[0, :, :, :]\n",
    "        pawns = 1 * np.sum(layer_board1[0, :, :])\n",
    "        rooks = 5 * np.sum(layer_board1[1, :, :])\n",
    "        minor = 3 * np.sum(layer_board1[2:4, :, :])\n",
    "        queen = 9 * np.sum(layer_board1[4, :, :])\n",
    "\n",
    "        maxscore = 40\n",
    "        material = pawns + rooks + minor + queen\n",
    "        board_value = self.color * material / maxscore\n",
    "        if noise:\n",
    "            added_noise = np.random.randn() / 1e3\n",
    "        return board_value + added_noise\n",
    "\n",
    "\n",
    "class Agent(object):\n",
    "\n",
    "    def __init__(self, lr=0.003, network='big'):\n",
    "        self.optimizer = RMSprop(lr=lr)\n",
    "        self.model = Model()\n",
    "        self.proportional_error = False\n",
    "        if network == 'simple':\n",
    "            self.init_simple_network()\n",
    "        elif network == 'super_simple':\n",
    "            self.init_super_simple_network()\n",
    "        elif network == 'alt':\n",
    "            self.init_altnet()\n",
    "        elif network == 'big':\n",
    "            self.init_bignet()\n",
    "        else:\n",
    "            self.init_network()\n",
    "\n",
    "    def fix_model(self):\n",
    "        \"\"\"\n",
    "        The fixed model is the model used for bootstrapping\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "\n",
    "        self.fixed_model = clone_model(self.model)\n",
    "        self.fixed_model.compile(optimizer=self.optimizer, loss='mse', metrics=['mae'])\n",
    "        self.fixed_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def init_network(self):\n",
    "        layer_state = Input(shape=(8, 8, 8), name='state')\n",
    "\n",
    "        openfile = Conv2D(3, (8, 1), padding='valid', activation='relu', name='fileconv')(layer_state)  # 3,8,1\n",
    "        openrank = Conv2D(3, (1, 8), padding='valid', activation='relu', name='rankconv')(layer_state)  # 3,1,8\n",
    "        quarters = Conv2D(3, (4, 4), padding='valid', activation='relu', name='quarterconv', strides=(4, 4))(\n",
    "            layer_state)  # 3,2,2\n",
    "        large = Conv2D(8, (6, 6), padding='valid', activation='relu', name='largeconv')(layer_state)  # 8,2,2\n",
    "\n",
    "        board1 = Conv2D(16, (3, 3), padding='valid', activation='relu', name='board1')(layer_state)  # 16,6,6\n",
    "        board2 = Conv2D(20, (3, 3), padding='valid', activation='relu', name='board2')(board1)  # 20,4,4\n",
    "        board3 = Conv2D(24, (3, 3), padding='valid', activation='relu', name='board3')(board2)  # 24,2,2\n",
    "\n",
    "        flat_file = Flatten()(openfile)\n",
    "        flat_rank = Flatten()(openrank)\n",
    "        flat_quarters = Flatten()(quarters)\n",
    "        flat_large = Flatten()(large)\n",
    "\n",
    "        flat_board = Flatten()(board1)\n",
    "        flat_board3 = Flatten()(board3)\n",
    "\n",
    "        dense1 = Concatenate(name='dense_bass')(\n",
    "            [flat_file, flat_rank, flat_quarters, flat_large, flat_board, flat_board3])\n",
    "        dropout1 = Dropout(rate=0.1)(dense1)\n",
    "        dense2 = Dense(128, activation='sigmoid')(dropout1)\n",
    "        dense3 = Dense(64, activation='sigmoid')(dense2)\n",
    "        dropout3 = Dropout(rate=0.1)(dense3, training=True)\n",
    "        dense4 = Dense(32, activation='sigmoid')(dropout3)\n",
    "        dropout4 = Dropout(rate=0.1)(dense4, training=True)\n",
    "\n",
    "        value_head = Dense(1)(dropout4)\n",
    "        self.model = Model(inputs=layer_state,\n",
    "                           outputs=[value_head])\n",
    "        self.model.compile(optimizer=self.optimizer,\n",
    "                           loss=[mean_squared_error]\n",
    "                           )\n",
    "\n",
    "    def init_simple_network(self):\n",
    "\n",
    "        layer_state = Input(shape=(8, 8, 8), name='state')\n",
    "        conv1 = Conv2D(8, (3, 3), activation='sigmoid')(layer_state)\n",
    "        conv2 = Conv2D(6, (3, 3), activation='sigmoid')(conv1)\n",
    "        conv3 = Conv2D(4, (3, 3), activation='sigmoid')(conv2)\n",
    "        flat4 = Flatten()(conv3)\n",
    "        dense5 = Dense(24, activation='sigmoid')(flat4)\n",
    "        dense6 = Dense(8, activation='sigmoid')(dense5)\n",
    "        value_head = Dense(1)(dense6)\n",
    "\n",
    "        self.model = Model(inputs=layer_state,\n",
    "                           outputs=value_head)\n",
    "        self.model.compile(optimizer=self.optimizer,\n",
    "                           loss=mean_squared_error\n",
    "                           )\n",
    "\n",
    "    def init_super_simple_network(self):\n",
    "        layer_state = Input(shape=(8, 8, 8), name='state')\n",
    "        conv1 = Conv2D(8, (3, 3), activation='sigmoid')(layer_state)\n",
    "        flat4 = Flatten()(conv1)\n",
    "        dense5 = Dense(10, activation='sigmoid')(flat4)\n",
    "        value_head = Dense(1)(dense5)\n",
    "\n",
    "        self.model = Model(inputs=layer_state,\n",
    "                           outputs=value_head)\n",
    "        self.model.compile(optimizer=self.optimizer,\n",
    "                           loss=mean_squared_error\n",
    "                           )\n",
    "\n",
    "    def init_altnet(self):\n",
    "        layer_state = Input(shape=(8, 8, 8), name='state')\n",
    "        conv1 = Conv2D(6, (1, 1), activation='sigmoid')(layer_state)\n",
    "        flat2 = Flatten()(conv1)\n",
    "        dense3 = Dense(128, activation='sigmoid')(flat2)\n",
    "\n",
    "        value_head = Dense(1)(dense3)\n",
    "\n",
    "        self.model = Model(inputs=layer_state,\n",
    "                           outputs=value_head)\n",
    "        self.model.compile(optimizer=self.optimizer,\n",
    "                           loss=mean_squared_error\n",
    "                           )\n",
    "\n",
    "    def init_bignet(self):\n",
    "        layer_state = Input(shape=(8, 8, 8), name='state')\n",
    "        conv_xs = Conv2D(4, (1, 1), activation='relu')(layer_state)\n",
    "        conv_s = Conv2D(8, (2, 2), strides=(1, 1), activation='relu')(layer_state)\n",
    "        conv_m = Conv2D(12, (3, 3), strides=(2, 2), activation='relu')(layer_state)\n",
    "        conv_l = Conv2D(16, (4, 4), strides=(2, 2), activation='relu')(layer_state)\n",
    "        conv_xl = Conv2D(20, (8, 8), activation='relu')(layer_state)\n",
    "        conv_rank = Conv2D(3, (1, 8), activation='relu')(layer_state)\n",
    "        conv_file = Conv2D(3, (8, 1), activation='relu')(layer_state)\n",
    "\n",
    "        f_xs = Flatten()(conv_xs)\n",
    "        f_s = Flatten()(conv_s)\n",
    "        f_m = Flatten()(conv_m)\n",
    "        f_l = Flatten()(conv_l)\n",
    "        f_xl = Flatten()(conv_xl)\n",
    "        f_r = Flatten()(conv_rank)\n",
    "        f_f = Flatten()(conv_file)\n",
    "\n",
    "        dense1 = Concatenate(name='dense_bass')([f_xs, f_s, f_m, f_l, f_xl, f_r, f_f])\n",
    "        dense2 = Dense(256, activation='sigmoid')(dense1)\n",
    "        dense3 = Dense(128, activation='sigmoid')(dense2)\n",
    "        dense4 = Dense(56, activation='sigmoid')(dense3)\n",
    "        dense5 = Dense(64, activation='sigmoid')(dense4)\n",
    "        dense6 = Dense(32, activation='sigmoid')(dense5)\n",
    "\n",
    "        value_head = Dense(1)(dense6)\n",
    "\n",
    "        self.model = Model(inputs=layer_state,\n",
    "                           outputs=value_head)\n",
    "        self.model.compile(optimizer=self.optimizer,\n",
    "                           loss=mean_squared_error\n",
    "                           )\n",
    "\n",
    "    def predict_distribution(self, states, batch_size=256):\n",
    "        \"\"\"\n",
    "        :param states: list of distinct states\n",
    "        :param n:  each state is predicted n times\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        predictions_per_state = int(batch_size / len(states))\n",
    "        state_batch = []\n",
    "        for state in states:\n",
    "            state_batch = state_batch + [state for x in range(predictions_per_state)]\n",
    "\n",
    "        state_batch = np.stack(state_batch, axis=0)\n",
    "        predictions = self.model.predict(state_batch)\n",
    "        predictions = predictions.reshape(len(states), predictions_per_state)\n",
    "        mean_pred = np.mean(predictions, axis=1)\n",
    "        std_pred = np.std(predictions, axis=1)\n",
    "        upper_bound = mean_pred + 2 * std_pred\n",
    "\n",
    "        return mean_pred, std_pred, upper_bound\n",
    "\n",
    "    def predict(self, board_layer):\n",
    "        return self.model.predict(board_layer)\n",
    "\n",
    "    def TD_update(self, states, rewards, sucstates, episode_active, gamma=0.9):\n",
    "        \"\"\"\n",
    "        Update the SARSA-network using samples from the minibatch\n",
    "        Args:\n",
    "            minibatch: list\n",
    "                The minibatch contains the states, moves, rewards and new states.\n",
    "\n",
    "        Returns:\n",
    "            td_errors: np.array\n",
    "                array of temporal difference errors\n",
    "\n",
    "        \"\"\"\n",
    "        suc_state_values = self.fixed_model.predict(sucstates)\n",
    "        V_target = np.array(rewards) + np.array(episode_active) * gamma * np.squeeze(suc_state_values)\n",
    "        # Perform a step of minibatch Gradient Descent.\n",
    "        self.model.fit(x=states, y=V_target, epochs=1, verbose=0)\n",
    "\n",
    "        V_state = self.model.predict(states)  # the expected future returns\n",
    "        td_errors = V_target - np.squeeze(V_state)\n",
    "\n",
    "        return td_errors\n",
    "\n",
    "    def MC_update(self, states, returns):\n",
    "        \"\"\"\n",
    "        Update network using a monte carlo playout\n",
    "        Args:\n",
    "            states: starting states\n",
    "            returns: discounted future rewards\n",
    "\n",
    "        Returns:\n",
    "            td_errors: np.array\n",
    "                array of temporal difference errors\n",
    "        \"\"\"\n",
    "        self.model.fit(x=states, y=returns, epochs=0, verbose=0)\n",
    "        V_state = np.squeeze(self.model.predict(states))\n",
    "        td_errors = returns - V_state\n",
    "\n",
    "        return td_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdbbb01",
   "metadata": {},
   "source": [
    "## Board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f16718cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = {}\n",
    "mapper[\"p\"] = 0\n",
    "mapper[\"r\"] = 1\n",
    "mapper[\"n\"] = 2\n",
    "mapper[\"b\"] = 3\n",
    "mapper[\"q\"] = 4\n",
    "mapper[\"k\"] = 5\n",
    "mapper[\"P\"] = 0\n",
    "mapper[\"R\"] = 1\n",
    "mapper[\"N\"] = 2\n",
    "mapper[\"B\"] = 3\n",
    "mapper[\"Q\"] = 4\n",
    "mapper[\"K\"] = 5\n",
    "\n",
    "\n",
    "class Board(object):\n",
    "\n",
    "    def __init__(self, opposing_agent, FEN=None, capture_reward_factor=0.01):\n",
    "        \"\"\"\n",
    "        Chess Board Environment\n",
    "        Args:\n",
    "            FEN: str\n",
    "                Starting FEN notation, if None then start in the default chess position\n",
    "            capture_reward_factor: float [0,inf]\n",
    "                reward for capturing a piece. Multiply material gain by this number. 0 for normal chess.\n",
    "        \"\"\"\n",
    "        self.FEN = FEN\n",
    "        self.capture_reward_factor = capture_reward_factor\n",
    "        self.board = chess.Board(self.FEN) if self.FEN else chess.Board()\n",
    "        self.layer_board = np.zeros(shape=(8, 8, 8))\n",
    "        self.init_layer_board()\n",
    "        self.opposing_agent = opposing_agent\n",
    "\n",
    "    def init_layer_board(self):\n",
    "        \"\"\"\n",
    "        Initalize the numerical representation of the environment\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        self.layer_board = np.zeros(shape=(8, 8, 8))\n",
    "        for i in range(64):\n",
    "            row = i // 8\n",
    "            col = i % 8\n",
    "            piece = self.board.piece_at(i)\n",
    "            if piece == None:\n",
    "                continue\n",
    "            elif piece.symbol().isupper():\n",
    "                sign = 1\n",
    "            else:\n",
    "                sign = -1\n",
    "            layer = mapper[piece.symbol()]\n",
    "            self.layer_board[layer, row, col] = sign\n",
    "            self.layer_board[6, :, :] = 1 / self.board.fullmove_number\n",
    "        if self.board.turn:\n",
    "            self.layer_board[6, 0, :] = 1\n",
    "        else:\n",
    "            self.layer_board[6, 0, :] = -1\n",
    "        self.layer_board[7, :, :] = 1\n",
    "\n",
    "    def update_layer_board(self, move=None):\n",
    "        self._prev_layer_board = self.layer_board.copy()\n",
    "        self.init_layer_board()\n",
    "\n",
    "    def pop_layer_board(self):\n",
    "        self.layer_board = self._prev_layer_board.copy()\n",
    "        self._prev_layer_board = None\n",
    "\n",
    "    def step(self, action, test=True):\n",
    "        \"\"\"\n",
    "        Run a step\n",
    "        Args:\n",
    "            action: python chess move\n",
    "        Returns:\n",
    "            epsiode end: Boolean\n",
    "                Whether the episode has ended\n",
    "            reward: float\n",
    "                Difference in material value after the move\n",
    "        \"\"\"\n",
    "        piece_balance_before = self.get_material_value()\n",
    "        self.board.push(action)\n",
    "        self.update_layer_board(action)\n",
    "        piece_balance_after = self.get_material_value()\n",
    "        auxiliary_reward = (piece_balance_after - piece_balance_before) * self.capture_reward_factor\n",
    "        result = self.board.result()\n",
    "        if result == \"*\":\n",
    "            reward = 0\n",
    "            episode_end = False\n",
    "        elif result == \"1-0\":\n",
    "            reward = 1\n",
    "            episode_end = True\n",
    "        elif result == \"0-1\":\n",
    "            reward = -1\n",
    "            episode_end = True\n",
    "        elif result == \"1/2-1/2\":\n",
    "            reward = 0\n",
    "            episode_end = True\n",
    "        reward += auxiliary_reward\n",
    "\n",
    "        return episode_end, reward\n",
    "\n",
    "    def get_random_action(self):\n",
    "        \"\"\"\n",
    "        Sample a random action\n",
    "        Returns: move\n",
    "            A legal python chess move.\n",
    "\n",
    "        \"\"\"\n",
    "        legal_moves = [x for x in self.board.generate_legal_moves()]\n",
    "        legal_moves = np.random.choice(legal_moves)\n",
    "        return legal_moves\n",
    "\n",
    "    def project_legal_moves(self):\n",
    "        \"\"\"\n",
    "        Create a mask of legal actions\n",
    "        Returns: np.ndarray with shape (64,64)\n",
    "        \"\"\"\n",
    "        self.action_space = np.zeros(shape=(64, 64))\n",
    "        moves = [[x.from_square, x.to_square] for x in self.board.generate_legal_moves()]\n",
    "        for move in moves:\n",
    "            self.action_space[move[0], move[1]] = 1\n",
    "        return self.action_space\n",
    "\n",
    "    def get_material_value(self):\n",
    "        \"\"\"\n",
    "        Sums up the material balance using Reinfield values\n",
    "        Returns: The material balance on the board\n",
    "        \"\"\"\n",
    "        pawns = 1 * np.sum(self.layer_board[0, :, :])\n",
    "        rooks = 5 * np.sum(self.layer_board[1, :, :])\n",
    "        minor = 3 * np.sum(self.layer_board[2:4, :, :])\n",
    "        queen = 9 * np.sum(self.layer_board[4, :, :])\n",
    "        return pawns + rooks + minor + queen\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        self.board = chess.Board(self.FEN) if self.FEN else chess.Board()\n",
    "        self.init_layer_board()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e253460e",
   "metadata": {},
   "source": [
    "## Learning Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "827dacb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, temperature=1):\n",
    "    return np.exp(x / temperature) / np.sum(np.exp(x / temperature))\n",
    "\n",
    "\n",
    "class Node(object):\n",
    "\n",
    "    def __init__(self, board=None, parent=None, gamma=0.9):\n",
    "        \"\"\"\n",
    "        Game Node for Monte Carlo Tree Search\n",
    "        Args:\n",
    "            board: the chess board\n",
    "            parent: the parent node\n",
    "            gamma: the discount factor\n",
    "        \"\"\"\n",
    "        self.children = {}  # Child nodes\n",
    "        self.board = board  # Chess board\n",
    "        self.parent = parent\n",
    "        self.values = []  # reward + Returns\n",
    "        self.gamma = gamma\n",
    "        self.starting_value = 0\n",
    "\n",
    "    def update_child(self, move, Returns):\n",
    "        \"\"\"\n",
    "        Update a child with a simulation result\n",
    "        Args:\n",
    "            move: The move that leads to the child\n",
    "            Returns: the reward of the move and subsequent returns\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        child = self.children[move]\n",
    "        child.values.append(Returns)\n",
    "\n",
    "    def update(self, Returns=None):\n",
    "        \"\"\"\n",
    "        Update a node with observed Returns\n",
    "        Args:\n",
    "            Returns: Future returns\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        if Returns:\n",
    "            self.values.append(Returns)\n",
    "\n",
    "    def select(self, color=1):\n",
    "        \"\"\"\n",
    "        Use Thompson sampling to select the best child node\n",
    "        Args:\n",
    "            color: Whether to select for white or black\n",
    "\n",
    "        Returns:\n",
    "            (node, move)\n",
    "            node: the selected node\n",
    "            move: the selected move\n",
    "        \"\"\"\n",
    "        assert color == 1 or color == -1, \"color has to be white (1) or black (-1)\"\n",
    "        if self.children:\n",
    "            max_sample = np.random.choice(color * np.array(self.values))\n",
    "            max_move = None\n",
    "            for move, child in self.children.items():\n",
    "                child_sample = np.random.choice(color * np.array(child.values))\n",
    "                if child_sample > max_sample:\n",
    "                    max_sample = child_sample\n",
    "                    max_move = move\n",
    "            if max_move:\n",
    "                return self.children[max_move], max_move\n",
    "            else:\n",
    "                return self, None\n",
    "        else:\n",
    "            return self, None\n",
    "\n",
    "    def simulate(self, model, env, depth=0, max_depth=4, random=False, temperature=1):\n",
    "        \"\"\"\n",
    "        Recursive Monte Carlo Playout\n",
    "        Args:\n",
    "            model: The model used for bootstrap estimation\n",
    "            env: the chess environment\n",
    "            depth: The recursion depth\n",
    "            max_depth: How deep to search\n",
    "            temperature: softmax temperature\n",
    "\n",
    "        Returns:\n",
    "            Playout result.\n",
    "        \"\"\"\n",
    "        board_in = env.board.fen()\n",
    "        if env.board.turn and random:\n",
    "            move = np.random.choice([x for x in env.board.generate_legal_moves()])\n",
    "        else:\n",
    "            successor_values = []\n",
    "            for move in env.board.generate_legal_moves():\n",
    "                episode_end, reward = env.step(move)\n",
    "                result = env.board.result()\n",
    "\n",
    "                if (result == \"1-0\" and env.board.turn) or (\n",
    "                        result == \"0-1\" and not env.board.turn):\n",
    "                    env.board.pop()\n",
    "                    env.init_layer_board()\n",
    "                    break\n",
    "                else:\n",
    "                    if env.board.turn:\n",
    "                        sucval = reward + self.gamma * np.squeeze(\n",
    "                            model.predict(np.expand_dims(env.layer_board, axis=0)))\n",
    "                    else:\n",
    "                        sucval = np.squeeze(env.opposing_agent.predict(np.expand_dims(env.layer_board, axis=0)))\n",
    "                    successor_values.append(sucval)\n",
    "                    env.board.pop()\n",
    "                    env.init_layer_board()\n",
    "\n",
    "            if not episode_end:\n",
    "                if env.board.turn:\n",
    "                    move_probas = softmax(np.array(successor_values), temperature=temperature)\n",
    "                    moves = [x for x in env.board.generate_legal_moves()]\n",
    "                else:\n",
    "                    move_probas = np.zeros(len(successor_values))\n",
    "                    move_probas[np.argmax(successor_values)] = 1\n",
    "                    moves = [x for x in env.board.generate_legal_moves()]\n",
    "                if len(moves) == 1:\n",
    "                    move = moves[0]\n",
    "                else:\n",
    "                    move = np.random.choice(moves, p=np.squeeze(move_probas))\n",
    "\n",
    "        episode_end, reward = env.step(move)\n",
    "\n",
    "        if episode_end:\n",
    "            Returns = reward\n",
    "        elif depth >= max_depth:  # Bootstrap the Monte Carlo Playout\n",
    "            Returns = reward + self.gamma * np.squeeze(model.predict(np.expand_dims(env.layer_board, axis=0)))\n",
    "        else:  # Recursively continue\n",
    "            Returns = reward + self.gamma * self.simulate(model, env, depth=depth + 1,temperature=temperature)\n",
    "\n",
    "        env.board.pop()\n",
    "        env.init_layer_board()\n",
    "\n",
    "        board_out = env.board.fen()\n",
    "        assert board_in == board_out\n",
    "\n",
    "        if depth == 0:\n",
    "            return Returns, move\n",
    "        else:\n",
    "            noise = np.random.randn() / 1e6\n",
    "            return Returns + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35503c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import gc\n",
    "\n",
    "\n",
    "def softmax(x, temperature=1):\n",
    "    return np.exp(x / temperature) / np.sum(np.exp(x / temperature))\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "\n",
    "class TD_search(object):\n",
    "\n",
    "    def __init__(self, env, agent, gamma=0.9, search_time=1, memsize=2000, batch_size=256, temperature=1):\n",
    "        \"\"\"\n",
    "        Chess algorithm that combines bootstrapped monte carlo tree search with Q Learning\n",
    "        Args:\n",
    "            env: RLC chess environment\n",
    "            agent: RLC chess agent\n",
    "            gamma: discount factor\n",
    "            search_time: maximum time spent doing tree search\n",
    "            memsize: Amount of training samples to keep in-memory\n",
    "            batch_size: Size of the training batches\n",
    "            temperature: softmax temperature for mcts\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.tree = Node(self.env)\n",
    "        self.gamma = gamma\n",
    "        self.memsize = memsize\n",
    "        self.batch_size = batch_size\n",
    "        self.temperature = temperature\n",
    "        self.reward_trace = []  # Keeps track of the rewards\n",
    "        self.piece_balance_trace = []  # Keep track of the material value on the board\n",
    "        self.ready = False  # Whether to start training\n",
    "        self.search_time = search_time\n",
    "        self.min_sim_count = 10\n",
    "\n",
    "        self.mem_state = np.zeros(shape=(1, 8, 8, 8))\n",
    "        self.mem_sucstate = np.zeros(shape=(1, 8, 8, 8))\n",
    "        self.mem_reward = np.zeros(shape=(1))\n",
    "        self.mem_error = np.zeros(shape=(1))\n",
    "        self.mem_episode_active = np.ones(shape=(1))\n",
    "\n",
    "    def learn(self, iters=40, c=5, timelimit_seconds=3600, maxiter=80):\n",
    "        \"\"\"\n",
    "        Start Reinforcement Learning Algorithm\n",
    "        Args:\n",
    "            iters: maximum amount of iterations to train\n",
    "            c: model update rate (once every C games)\n",
    "            timelimit_seconds: maximum training time\n",
    "            maxiter: Maximum duration of a game, in halfmoves\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        starttime = time.time()\n",
    "        for k in range(iters):\n",
    "            self.env.reset()\n",
    "            if k % c == 0:\n",
    "                self.agent.fix_model()\n",
    "                print(\"iter\", k)\n",
    "            if k > c:\n",
    "                self.ready = True\n",
    "            self.play_game(k, maxiter=maxiter)\n",
    "            if starttime + timelimit_seconds < time.time():\n",
    "                break\n",
    "        return self.env.board\n",
    "\n",
    "    def play_game(self, k, maxiter=80):\n",
    "        \"\"\"\n",
    "        Play a chess game and learn from it\n",
    "        Args:\n",
    "            k: the play iteration number\n",
    "            maxiter: maximum duration of the game (halfmoves)\n",
    "\n",
    "        Returns:\n",
    "            board: Chess environment on terminal state\n",
    "        \"\"\"\n",
    "        episode_end = False\n",
    "        turncount = 0\n",
    "        tree = Node(self.env.board, gamma=self.gamma)  # Initialize the game tree\n",
    "\n",
    "        # Play a game of chess\n",
    "        while not episode_end:\n",
    "            state = np.expand_dims(self.env.layer_board.copy(), axis=0)\n",
    "            state_value = self.agent.predict(state)\n",
    "\n",
    "            # White's turn involves tree-search\n",
    "            if self.env.board.turn:\n",
    "\n",
    "                # Do a Monte Carlo Tree Search after game iteration k\n",
    "                start_mcts_after = -1\n",
    "                if k > start_mcts_after:\n",
    "                    tree = self.mcts(tree)\n",
    "                    # Step the best move\n",
    "                    max_move = None\n",
    "                    max_value = np.NINF\n",
    "                    for move, child in tree.children.items():\n",
    "                        sampled_value = np.mean(child.values)\n",
    "                        if sampled_value > max_value:\n",
    "                            max_value = sampled_value\n",
    "                            max_move = move\n",
    "                else:\n",
    "                    max_move = np.random.choice([move for move in self.env.board.generate_legal_moves()])\n",
    "\n",
    "            # Black's turn is myopic\n",
    "            else:\n",
    "                max_move = None\n",
    "                max_value = np.NINF\n",
    "                for move in self.env.board.generate_legal_moves():\n",
    "                    self.env.step(move)\n",
    "                    if self.env.board.result() == \"0-1\":\n",
    "                        max_move = move\n",
    "                        self.env.board.pop()\n",
    "                        self.env.init_layer_board()\n",
    "                        break\n",
    "                    successor_state_value_opponent = self.env.opposing_agent.predict(\n",
    "                        np.expand_dims(self.env.layer_board, axis=0))\n",
    "                    if successor_state_value_opponent > max_value:\n",
    "                        max_move = move\n",
    "                        max_value = successor_state_value_opponent\n",
    "\n",
    "                    self.env.board.pop()\n",
    "                    self.env.init_layer_board()\n",
    "\n",
    "            if not (self.env.board.turn and max_move not in tree.children.keys()) or not k > start_mcts_after:\n",
    "                tree.children[max_move] = Node(gamma=0.9, parent=tree)\n",
    "\n",
    "            episode_end, reward = self.env.step(max_move)\n",
    "\n",
    "            tree = tree.children[max_move]\n",
    "            tree.parent = None\n",
    "            gc.collect()\n",
    "\n",
    "            sucstate = np.expand_dims(self.env.layer_board, axis=0)\n",
    "            new_state_value = self.agent.predict(sucstate)\n",
    "\n",
    "            error = reward + self.gamma * new_state_value - state_value\n",
    "            error = float(np.squeeze(error))\n",
    "\n",
    "            turncount += 1\n",
    "            if turncount > maxiter and not episode_end:\n",
    "                episode_end = True\n",
    "\n",
    "            episode_active = 0 if episode_end else 1\n",
    "\n",
    "            # construct training sample state, prediction, error\n",
    "            self.mem_state = np.append(self.mem_state, state, axis=0)\n",
    "            self.mem_reward = np.append(self.mem_reward, reward)\n",
    "            self.mem_sucstate = np.append(self.mem_sucstate, sucstate, axis=0)\n",
    "            self.mem_error = np.append(self.mem_error, error)\n",
    "            self.reward_trace = np.append(self.reward_trace, reward)\n",
    "            self.mem_episode_active = np.append(self.mem_episode_active, episode_active)\n",
    "\n",
    "            if self.mem_state.shape[0] > self.memsize:\n",
    "                self.mem_state = self.mem_state[1:]\n",
    "                self.mem_reward = self.mem_reward[1:]\n",
    "                self.mem_sucstate = self.mem_sucstate[1:]\n",
    "                self.mem_error = self.mem_error[1:]\n",
    "                self.mem_episode_active = self.mem_episode_active[1:]\n",
    "                gc.collect()\n",
    "\n",
    "            if turncount % 10 == 0:\n",
    "                self.update_agent()\n",
    "\n",
    "        piece_balance = self.env.get_material_value()\n",
    "        self.piece_balance_trace.append(piece_balance)\n",
    "        print(\"game ended with result\", reward, \"and material balance\", piece_balance, \"in\", turncount, \"halfmoves\")\n",
    "\n",
    "        return self.env.board\n",
    "\n",
    "    def update_agent(self):\n",
    "        \"\"\"\n",
    "        Update the Agent with TD learning\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        if self.ready:\n",
    "            choice_indices, states, rewards, sucstates, episode_active = self.get_minibatch()\n",
    "            td_errors = self.agent.TD_update(states, rewards, sucstates, episode_active, gamma=self.gamma)\n",
    "            self.mem_error[choice_indices.tolist()] = td_errors\n",
    "\n",
    "    def get_minibatch(self, prioritized=True):\n",
    "        \"\"\"\n",
    "        Get a mini batch of experience\n",
    "        Args:\n",
    "            prioritized:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        if prioritized:\n",
    "            sampling_priorities = np.abs(self.mem_error) + 1e-9\n",
    "        else:\n",
    "            sampling_priorities = np.ones(shape=self.mem_error.shape)\n",
    "        sampling_probs = sampling_priorities / np.sum(sampling_priorities)\n",
    "        sample_indices = [x for x in range(self.mem_state.shape[0])]\n",
    "        choice_indices = np.random.choice(sample_indices,\n",
    "                                          min(self.mem_state.shape[0],\n",
    "                                              self.batch_size),\n",
    "                                          p=np.squeeze(sampling_probs),\n",
    "                                          replace=False\n",
    "                                          )\n",
    "        states = self.mem_state[choice_indices]\n",
    "        rewards = self.mem_reward[choice_indices]\n",
    "        sucstates = self.mem_sucstate[choice_indices]\n",
    "        episode_active = self.mem_episode_active[choice_indices]\n",
    "\n",
    "        return choice_indices, states, rewards, sucstates, episode_active\n",
    "\n",
    "    def mcts(self, node):\n",
    "        \"\"\"\n",
    "        Run Monte Carlo Tree Search\n",
    "        Args:\n",
    "            node: A game state node object\n",
    "\n",
    "        Returns:\n",
    "            the node with playout sims\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        starttime = time.time()\n",
    "        sim_count = 0\n",
    "        board_in = self.env.board.fen()\n",
    "\n",
    "        # First make a prediction for each child state\n",
    "        for move in self.env.board.generate_legal_moves():\n",
    "            if move not in node.children.keys():\n",
    "                node.children[move] = Node(self.env.board, parent=node)\n",
    "\n",
    "            episode_end, reward = self.env.step(move)\n",
    "\n",
    "            if episode_end:\n",
    "                successor_state_value = 0\n",
    "            else:\n",
    "                successor_state_value = np.squeeze(\n",
    "                    self.agent.model.predict(np.expand_dims(self.env.layer_board, axis=0))\n",
    "                )\n",
    "\n",
    "            child_value = reward + self.gamma * successor_state_value\n",
    "\n",
    "            node.update_child(move, child_value)\n",
    "            self.env.board.pop()\n",
    "            self.env.init_layer_board()\n",
    "        if not node.values:\n",
    "            node.values = [0]\n",
    "\n",
    "        while starttime + self.search_time > time.time() or sim_count < self.min_sim_count:\n",
    "            depth = 0\n",
    "            color = 1\n",
    "            node_rewards = []\n",
    "\n",
    "            # Select the best node from where to start MCTS\n",
    "            while node.children:\n",
    "                node, move = node.select(color=color)\n",
    "                if not move:\n",
    "                    # No move means that the node selects itself, not a child node.\n",
    "                    break\n",
    "                else:\n",
    "                    depth += 1\n",
    "                    color = color * -1  # switch color\n",
    "                    episode_end, reward = self.env.step(move)  # Update the environment to reflect the node\n",
    "                    node_rewards.append(reward)\n",
    "                    # Check best node is terminal\n",
    "\n",
    "                    if self.env.board.result() == \"1-0\" and depth == 1:  # -> Direct win for white, no need for mcts.\n",
    "                        self.env.board.pop()\n",
    "                        self.env.init_layer_board()\n",
    "                        node.update(1)\n",
    "                        node = node.parent\n",
    "                        return node\n",
    "                    elif episode_end:  # -> if the explored tree leads to a terminal state, simulate from root.\n",
    "                        while node.parent:\n",
    "                            self.env.board.pop()\n",
    "                            self.env.init_layer_board()\n",
    "                            node = node.parent\n",
    "                        break\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "            # Expand the game tree with a simulation\n",
    "            Returns, move = node.simulate(self.agent.fixed_model,\n",
    "                                          self.env,\n",
    "                                          temperature=self.temperature,\n",
    "                                          depth=0)\n",
    "            self.env.init_layer_board()\n",
    "\n",
    "            if move not in node.children.keys():\n",
    "                node.children[move] = Node(self.env.board, parent=node)\n",
    "\n",
    "            node.update_child(move, Returns)\n",
    "\n",
    "            # Return to root node and backpropagate Returns\n",
    "            while node.parent:\n",
    "                latest_reward = node_rewards.pop(-1)\n",
    "                Returns = latest_reward + self.gamma * Returns\n",
    "                node.update(Returns)\n",
    "                node = node.parent\n",
    "\n",
    "                self.env.board.pop()\n",
    "                self.env.init_layer_board()\n",
    "            sim_count += 1\n",
    "\n",
    "        board_out = self.env.board.fen()\n",
    "        assert board_in == board_out\n",
    "\n",
    "        return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2ef0a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soner\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "state (InputLayer)              [(None, 8, 8, 8)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 8, 8, 4)      36          state[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 7, 7, 8)      264         state[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 3, 3, 12)     876         state[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 3, 3, 16)     2064        state[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 1, 1, 20)     10260       state[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 8, 1, 3)      195         state[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 1, 8, 3)      195         state[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 256)          0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 392)          0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 108)          0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 144)          0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 20)           0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 24)           0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 24)           0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_bass (Concatenate)        (None, 968)          0           flatten_7[0][0]                  \n",
      "                                                                 flatten_8[0][0]                  \n",
      "                                                                 flatten_9[0][0]                  \n",
      "                                                                 flatten_10[0][0]                 \n",
      "                                                                 flatten_11[0][0]                 \n",
      "                                                                 flatten_12[0][0]                 \n",
      "                                                                 flatten_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 256)          248064      dense_bass[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 128)          32896       dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 56)           7224        dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 64)           3648        dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 32)           2080        dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 1)            33          dense_10[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 307,835\n",
      "Trainable params: 307,835\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "opponent = GreedyAgent()\n",
    "env = Board(opponent, FEN=None)\n",
    "player = Agent(lr=0.0005,network='big')\n",
    "learner = TD_search(env, player,gamma=0.9,search_time=0.9)\n",
    "node = Node(learner.env.board, gamma=learner.gamma)\n",
    "player.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "96e0603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 80000  # maximum number of iterations\n",
    "timelimit = 200000 # maximum time for learning\n",
    "network_replacement_interval = 10  # For the stability of the nearal network updates, the network is not continuously replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8fd2c203",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0\n",
      "game ended with result 0.0 and material balance -19.0 in 140 halfmoves\n",
      "game ended with result 0.05 and material balance 0.0 in 261 halfmoves\n",
      "game ended with result 0.01 and material balance -3.0 in 215 halfmoves\n",
      "game ended with result -1.0 and material balance -33.0 in 72 halfmoves\n",
      "game ended with result 0.09 and material balance 0.0 in 225 halfmoves\n",
      "game ended with result 0.09 and material balance 0.0 in 277 halfmoves\n",
      "game ended with result 0.09 and material balance 0.0 in 307 halfmoves\n",
      "game ended with result 0.01 and material balance -3.0 in 153 halfmoves\n",
      "game ended with result 0.01 and material balance 0.0 in 191 halfmoves\n",
      "game ended with result 0.01 and material balance 0.0 in 245 halfmoves\n",
      "iter 10\n",
      "game ended with result -1.13 and material balance -16.0 in 28 halfmoves\n",
      "game ended with result 0.01 and material balance 0.0 in 149 halfmoves\n",
      "game ended with result -1.0 and material balance -18.0 in 86 halfmoves\n",
      "game ended with result -1.0 and material balance -23.0 in 72 halfmoves\n",
      "game ended with result -0.05 and material balance -12.0 in 136 halfmoves\n",
      "game ended with result 0.09 and material balance -3.0 in 125 halfmoves\n",
      "game ended with result 1.0 and material balance -1.0 in 13 halfmoves\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-90e6773ceea1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlearner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_iters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtimelimit_seconds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimelimit\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnetwork_replacement_interval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mn_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-32-2929ed1408d6>\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, iters, c, timelimit_seconds, maxiter)\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mready\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplay_game\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxiter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstarttime\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtimelimit_seconds\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-2929ed1408d6>\u001b[0m in \u001b[0;36mplay_game\u001b[1;34m(self, k, maxiter)\u001b[0m\n\u001b[0;32m     94\u001b[0m                 \u001b[0mstart_mcts_after\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mstart_mcts_after\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m                     \u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmcts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m                     \u001b[1;31m# Step the best move\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m                     \u001b[0mmax_move\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-2929ed1408d6>\u001b[0m in \u001b[0;36mmcts\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    285\u001b[0m                                           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m                                           \u001b[0mtemperature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m                                           depth=0)\n\u001b[0m\u001b[0;32m    288\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_layer_board\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-55327183f848>\u001b[0m in \u001b[0;36msimulate\u001b[1;34m(self, model, env, depth, max_depth, random, temperature)\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_board\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Recursively continue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m             \u001b[0mReturns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimulate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdepth\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-55327183f848>\u001b[0m in \u001b[0;36msimulate\u001b[1;34m(self, model, env, depth, max_depth, random, temperature)\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_board\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Recursively continue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m             \u001b[0mReturns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimulate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdepth\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-55327183f848>\u001b[0m in \u001b[0;36msimulate\u001b[1;34m(self, model, env, depth, max_depth, random, temperature)\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_board\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Recursively continue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m             \u001b[0mReturns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimulate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdepth\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-55327183f848>\u001b[0m in \u001b[0;36msimulate\u001b[1;34m(self, model, env, depth, max_depth, random, temperature)\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_board\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Recursively continue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m             \u001b[0mReturns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimulate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdepth\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-55327183f848>\u001b[0m in \u001b[0;36msimulate\u001b[1;34m(self, model, env, depth, max_depth, random, temperature)\u001b[0m\n\u001b[0;32m    102\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mturn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m                         sucval = reward + self.gamma * np.squeeze(\n\u001b[1;32m--> 104\u001b[1;33m                             model.predict(np.expand_dims(env.layer_board, axis=0)))\n\u001b[0m\u001b[0;32m    105\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m                         \u001b[0msucval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopposing_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_board\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1745\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1746\u001b[0m       \u001b[0mbatch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1747\u001b[1;33m       \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Single epoch.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1748\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1749\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m       \u001b[0mdata_iterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1181\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    409\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    694\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0melement_spec\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 696\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    697\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    717\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[1;32m--> 719\u001b[1;33m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    720\u001b[0m       \u001b[1;31m# Delete the resource when this object is deleted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m       self._resource_deleter = IteratorResourceDeleter(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3118\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3119\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m-> 3120\u001b[1;33m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[0m\u001b[0;32m   3121\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3122\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learner.learn(iters=n_iters,timelimit_seconds=timelimit,c=network_replacement_interval, maxiter=2*n_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9001658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_smooth = pd.DataFrame(learner.reward_trace)\n",
    "reward_smooth.rolling(window=1000,min_periods=1000).mean().plot(figsize=(16,9),title='average reward over the last 1000 steps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c53622",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_smooth = pd.DataFrame(learner.piece_balance_trace)\n",
    "reward_smooth.rolling(window=50,min_periods=50).mean().plot(figsize=(16,9),title='average piece balance over the last 50 episodes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ead1803d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.env.reset()\n",
    "learner.search_time = 60\n",
    "learner.temperature = 1/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e76f847f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-52177d83e363>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlearner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplay_game\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmaxiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-32-2929ed1408d6>\u001b[0m in \u001b[0;36mplay_game\u001b[1;34m(self, k, maxiter)\u001b[0m\n\u001b[0;32m     94\u001b[0m                 \u001b[0mstart_mcts_after\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mstart_mcts_after\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m                     \u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmcts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m                     \u001b[1;31m# Step the best move\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m                     \u001b[0mmax_move\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-2929ed1408d6>\u001b[0m in \u001b[0;36mmcts\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    285\u001b[0m                                           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m                                           \u001b[0mtemperature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m                                           depth=0)\n\u001b[0m\u001b[0;32m    288\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_layer_board\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-55327183f848>\u001b[0m in \u001b[0;36msimulate\u001b[1;34m(self, model, env, depth, max_depth, random, temperature)\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_board\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Recursively continue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m             \u001b[0mReturns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimulate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdepth\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-55327183f848>\u001b[0m in \u001b[0;36msimulate\u001b[1;34m(self, model, env, depth, max_depth, random, temperature)\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_board\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Recursively continue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m             \u001b[0mReturns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimulate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdepth\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-55327183f848>\u001b[0m in \u001b[0;36msimulate\u001b[1;34m(self, model, env, depth, max_depth, random, temperature)\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_board\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Recursively continue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m             \u001b[0mReturns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimulate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdepth\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-55327183f848>\u001b[0m in \u001b[0;36msimulate\u001b[1;34m(self, model, env, depth, max_depth, random, temperature)\u001b[0m\n\u001b[0;32m    102\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mturn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m                         sucval = reward + self.gamma * np.squeeze(\n\u001b[1;32m--> 104\u001b[1;33m                             model.predict(np.expand_dims(env.layer_board, axis=0)))\n\u001b[0m\u001b[0;32m    105\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m                         \u001b[0msucval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopposing_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_board\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1728\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1729\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1730\u001b[1;33m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[0;32m   1731\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1732\u001b[0m       \u001b[1;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1381\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_cluster_coordinator\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1382\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1383\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1148\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1150\u001b[1;33m         model=model)\n\u001b[0m\u001b[0;32m   1151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1152\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mflat_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m     \u001b[0mindices_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[1;34m(self, map_func)\u001b[0m\n\u001b[0;32m   1901\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1902\u001b[0m     \"\"\"\n\u001b[1;32m-> 1903\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mFlatMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1904\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1905\u001b[0m   def interleave(self,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[0;32m   5061\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5062\u001b[0m     self._map_func = StructuredFunctionWrapper(\n\u001b[1;32m-> 5063\u001b[1;33m         map_func, self._transformation_name(), dataset=input_dataset)\n\u001b[0m\u001b[0;32m   5064\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDatasetSpec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5065\u001b[0m       raise TypeError(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   4216\u001b[0m         \u001b[0mfn_factory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrace_tf_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefun_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4218\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4219\u001b[0m     \u001b[1;31m# There is no graph to add in eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4220\u001b[0m     \u001b[0madd_to_graph\u001b[0m \u001b[1;33m&=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3149\u001b[0m     \"\"\"\n\u001b[0;32m   3150\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[1;32m-> 3151\u001b[1;33m         *args, **kwargs)\n\u001b[0m\u001b[0;32m   3152\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3153\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3114\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3115\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3116\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3117\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3118\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3463\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3306\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3307\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3308\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   3309\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3310\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[0;32m    922\u001b[0m       \u001b[0mkwarg_shapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m     func_args = _get_defun_inputs_from_args(\n\u001b[1;32m--> 924\u001b[1;33m         args, arg_names, flat_shapes=arg_shapes)\n\u001b[0m\u001b[0;32m    925\u001b[0m     func_kwargs = _get_defun_inputs_from_kwargs(\n\u001b[0;32m    926\u001b[0m         kwargs, flat_shapes=kwarg_shapes)\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36m_get_defun_inputs_from_args\u001b[1;34m(args, names, flat_shapes)\u001b[0m\n\u001b[0;32m   1158\u001b[0m   \u001b[1;34m\"\"\"Maps Python function positional args to graph-construction inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1159\u001b[0m   return _get_defun_inputs(\n\u001b[1;32m-> 1160\u001b[1;33m       args, names, structure=args, flat_shapes=flat_shapes)\n\u001b[0m\u001b[0;32m   1161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36m_get_defun_inputs\u001b[1;34m(args, names, structure, flat_shapes)\u001b[0m\n\u001b[0;32m   1232\u001b[0m           placeholder = graph_placeholder(\n\u001b[0;32m   1233\u001b[0m               \u001b[0marg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplaceholder_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1234\u001b[1;33m               name=requested_name)\n\u001b[0m\u001b[0;32m   1235\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1236\u001b[0m           \u001b[1;31m# Sometimes parameter names are not valid op names, so fall back to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\graph_only_ops.py\u001b[0m in \u001b[0;36mgraph_placeholder\u001b[1;34m(dtype, shape, name)\u001b[0m\n\u001b[0;32m     38\u001b[0m   op = g._create_op_internal(  # pylint: disable=protected-access\n\u001b[0;32m     39\u001b[0m       \u001b[1;34m\"Placeholder\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m       attrs=attrs, name=name)\n\u001b[0m\u001b[0;32m     41\u001b[0m   \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mop_callbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_invoke_op_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    599\u001b[0m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[0;32m    600\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 601\u001b[1;33m         compute_device)\n\u001b[0m\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   3567\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3568\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3569\u001b[1;33m           op_def=op_def)\n\u001b[0m\u001b[0;32m   3570\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3571\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   2040\u001b[0m         \u001b[0mop_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_op_def\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2041\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, inputs,\n\u001b[1;32m-> 2042\u001b[1;33m                                 control_input_ops, op_def)\n\u001b[0m\u001b[0;32m   2043\u001b[0m       \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2044\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[0;32m   1878\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1879\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1880\u001b[1;33m     \u001b[0mc_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1881\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1882\u001b[0m     \u001b[1;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learner.play_game(n_iters,maxiter=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35a415a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pgn = Game.from_board(learner.env.board)\n",
    "with open(\"rlc_pgn_v4\",\"w\") as log:\n",
    "    log.write(str(pgn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1f2431a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.agent.model.save('RLC_model_v4_5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca00a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
